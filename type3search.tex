\documentclass{ltugboat}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{ifpdf}
\def\PDF{\acro{PDF}}
\def\dvips{\texttt{dvips}}
\def\ps2pdf{\texttt{ps2pdf}}
\ifpdf
\usepackage[breaklinks,hidelinks,pdfa]{hyperref}
\else
\usepackage{url}
\fi

\title{Type 3 Fonts and \PDF\ Search}

\author{Tomas Rokicki}
\address{Palo Alto, California \\ United States}
\netaddress{rokicki@gmail.com}

\begin{document}
\maketitle

\begin{abstract}
The web is filled with \PDF\ files generated from the output of
\dvips\ using bitmapped fonts that are not properly searchable,
indexable, or accessible.  While a full solution is challenging,
only minimal changes are required to \dvips\ to support
English language text, changes that are at least two decades overdue.
I will show what the changes are as well as discuss their
limitations.
\end{abstract}

\section{Introduction}

The Type 3 fonts generated by \dvips\ for bitmapped fonts
lack a reasonable encoding vector, and this prevents \PDF\ 
viewers from interpreting those glyphs as text.  This in turn
prevents text search, copy and paste, screen readers, and
search engine indexing from working correctly.  Fixing this is
easy, at least for English text, and comes with no significant
cost.

This small change is not nearly a full solution to create
accessible \PDF\ multilingual documents, but as it will
improve the situation for some common use cases it is still
a valid and important change.

We describe how we generated reasonable encoding vectors
for common \MF\ fonts, how \dvips\ finds these
encoding vectors and embeds them in the Postscript file, and
how the current implementation allows for future
experimentation and enhancement.

\section{A Little History}

When \dvips\ was originally written in 1986, the lone
Postscript interpreter on hand was an Apple Laserwriter with
170K available memory.  I treated Postscript as just a form of
compression for the page bitmap, doing the bare minimum to
satisfy the requirements for Level 1 Type 3 fonts.  One of
those requirements was to supply an \texttt{/Encoding} vector,
despite the fact that at the time, the vector was completely
unnecessary in rendering the glyphs.  Not considering that people
might someday use that encoding vector for glyph identification,
I generated a semantically nonsensical but syntactically acceptable
vector for all bitmapped fonts on that fateful day in 1986, and
this vector remains to this day, subverting any attempt to search
copy, or use screen readers.

Just replacing this encoding vector with something more
reasonable remedies this problem for the most part, for
English-language text.

\section{A Sample}

The following \TeX\ file, cribbed from \texttt{testfont.tex}
but using only a single font, will be used as illustration.

\begin{verbatim}[\small]
\hsize=3in \noindent
On November 14, 1885, Senator \& Mrs.~Leland
Stanford called together at their San
Francisco mansion the 24~prominent men who
had been chosen as the first trustees of The
Leland Stanford Junior University.
?`But aren't Kafka's Schlo{\ss} and {\AE}sop's
{\OE}uvres often na{\"\i}ve  vis-\`a-vis the
d{\ae}monic ph{\oe}nix's official r\^ole
in fluffy souffl\'es?
\bye
\end{verbatim}
\noindent
When you run this through \TeX\ and then \dvips\
(giving the \texttt{-V1} option to enforce bitmapped
and not Type 1 fonts), and then process the PostScript
result with \ps2pdf, the resulting \PDF\ does not
support text search in most \PDF\ viewers.  In Acrobat
with copy and paste it almost works; for some reason
the c's are dropped throughout (San Francisco becomes
San Fran is o), and ligatures don't work.  In OSX
preview, selecting text appears to fail (it
actually works, but the selection boxes are too small
to see that anything has actually been selected) and
no characters are recognized as alphabetic.  In Chrome
\PDF\ Preview, selecting text gives a random note
appearance with each word separately selected by its
bounding box and no alphabetic characters recognized.

Conversely, when you process the file with Type 1
fonts, all text functions perform normally, except that
accented characters are detected as two separate
characters (the accent and the base character).  The
critical difference is not Type 3 (bitmaps) versus
Type 1 (outline fonts), but rather the lack of a
sensible encoding vector in the Type 3 font.

\section{First Attempts and Failure}

If we manually copy the Encoding vector from
the output of dvips using Type 1 fonts and put that
in the font definition for the Type 3 fonts, the situation
improves; now Adobe Acrobat properly supports text
functions (including ligatures but not accented
characters).  The other \PDF\ viewers now recognize
alphabetic characters, but they still have a number of
other problems.

With OSX Preview, if you use command-A
(to select all the text) and then command-C (to copy
it), and you copy the result into a text editor (or
a word processing program ``without formatting''),
you get the following mismash of text:

\begin{quote}
On Novemb er 14, 1885, Senator \& Mrs. Leland
Stanford called
mansion the 24
together at their San Francisco
prominent men who had
b een cho-
Stanford
sen as the first trustees of The Leland
Junior
{\AE}sop’s
University. ?`But aren’t Kafka’s Schlo\ss\ and
{\OE}uvres often na"ıve vis-`a-vis the dæmonic
ph{\oe}nix’s official rˆole in fluffy souffl'es?
\end{quote}
\noindent
If you look carefully you'll notice some surprising
and substantial word reordering!  What could
be going on?

\section{Refinements and Success}

All \PDF\ viewers use some heuristics
to turn a group of rendered glyphs into a text stream.
The heuristics differ significantly from viewer to
viewer.  The most important heuristic appears to be
that interpreting horizontal escapement into one of
three categories: kerns, word breaks, and column
gutters.  To satisfy the \PDF\ viewers I had access to,
I had to do two additional modifications to each bitmapped
font.

First, I had to adjust the font coordinate system.
The default Adobe font coordinate system has 1000 units
to the em, while the original
\dvips\ uses a coordinate system that had one unit to the
pixel both for the page and for the font, and doesn't use
the PostScript \texttt{scalefont} primitive.  But not using
\texttt{scalefont} apparently made some viewers think all the
fonts were just one point high, and they used heuristics
appropriate for such a font.  By providing a font matrix
that was more in line with conventional fonts, and using
\texttt{scalefont}, \PDF\ viewers made better guesses about
the appropriate font metrics used in their heuristics.

Second, I had to calculate and provide a real font bounding
box.  The original \dvips\ code gave all zeros for the font
bounding box, which is specifically allowed by Postscript,
but this confused some \PDF\ viewers.  So I added code to
calculate the actual bounding box for the font from the
glyph definitions and provided that.

With these adjustments, using \dvips\ with bitmapped fonts
and \ps2pdf\ generates \PDF\ files that can be properly
searched---for English language text.

\section{Other Languages: No Success}

I would have liked things to work with other languages as
well, but was not able to get it to work.  Clearly the
\PDF\ viewers are recognizing characters by the glyph
names, but this appears to work only with a small set of
glyph names.  I hoped that those listed in the official
Adobe Glyph List would work, but in my experiments they
(for the most part) did not.  I also tried Unicode code
point glyph names such as \texttt{/uni1234} and \texttt{/u1234}
but neither of these formats worked in the \PDF\ viewers
I tried.  I also experimented with adding a \texttt{cmap}
to the font, with no success, and even tried some lightly
documented GhostView hacks, but was only able to achieve
distressingly partial success for most non-Roman characters.

Then there are accents, and more generally, virtual fonts.
With standard seven-bit encoding, accents are generally
rendered as two separate characters, where the \PDF\ viewer
only expects to see a single composite character.  Further,
the entire virtual font layer would need to be mapped in
some fashion, as the \PDF\ contains the physical glyphs
that are often combined in some way to provide the semantic
characters.  Supporting this would have required significantly
more work, and there are already efforts in this direction
from people much more knowledgeable and capable than I am.

The lack of success for other languages diminishes these
proposed changes, but the changes are still important as they
do provide reasonable support for English-language documents.
The implementation provides for some future experimentation and
extension, as will be described.

\section{Finding Font Encodings}

In order to provide more than a proof of concept, I had to
locate appropriate glyph names for the fonts provided with
\TeX Live, as well as give a way for end users to add their
own glyph names for their own personal fonts.

Over the years nearly all (if not all) of the \MF\ fonts
provided with \TeX Live have been translated to the Type 1
format by others, and as part of that process reasonable
encoding vectors have been created for the glyphs.  I
decided to completely leverage this work, so I wrote a
script that located all the \MF\ sources in the \TeX Live
distribution, all the corresponding Type 1 fonts, and any
encoding files used in the relevant \texttt{psfonts.map}
file.  A big Perl script chewed on all of this, extracting
encoding vectors and naming files appropriately.  Some of
the encoding vectors use glyph names that are not particularly
useful, and some use glyph names based on Unicode code
points that are not currently recognized by the \PDF\ viewers
I tried, but I did not want to edit the names in any way;
I aimed for functional equivalence to using the Type 1
fonts.  If improvements are made to the Type 1 font glyph
names, or to the \PDF\ viewers, I wanted to be able to pick
up those improvements.

I considered having \dvips\ read the encoding vectors
directly from the Type 1 fonts, rather than extracting them
and storing them elsewhere, but decided against this; I wanted
\dvips\ to use appropriate glyph names even if the Type 1
fonts didn't exist at all.  This does introduce a certain
redundancy which can potentially lead to an inconsistency in
the glyph names, but the fonts are currently mostly stable, and
glyph name extraction process can be
repeated as needed if substantive changes are made.

\section{Storing and Distributing Encodings}

After scanning all of the relevant \MF\ files and corresponding
Type 1 files, I found there were 2885 fonts; storing the
encodings separately one per font would require an additional
2,885 files in \TeX Live, occupying about 5 megabytes.  I felt
this was excessive for the functionality added.

Karl Berry suggested combining all the encodings into a single
file, along with a list of fonts using any particular encoding.
Since there were only 138 distinct encodings, this gave
tremendous compression, letting me store all of the encodings
for all of the fonts in a single file of size 183K.  This also
enabled me to distribute a simple test Perl script that mocked
the changes so people could try them out without updating their
\TeX\ installation.

This combined file, called \texttt{dvips-all.enc}, just provides
the default encoding used by the 2885 distributed \TeX Live
\MF\ fonts.  In every case that \dvips\ looks for an encoding,
i.e., for \texttt{cmr10}, it first searches for \texttt{dvips-cmr10.enc}
and only falls back to the information in the combined file if
that file is not found.  This permits users to provide their
own encoding for local \MF\ fonts, as well.

The format of the encoding file is slightly different from that
of other encoding files in the \TeX Live distribution.  The
encoding file should be a Postscript fragment that pushes a
single object on the operand stack.  That object should either be a
legitimate encoding vector consisting of an array of 256
Postscript names, or it should be a procedure that pushes
such an encoding vector.  It should not attempt to define
the \texttt{/Encoding} name in the current dictionary, as some other
encoding file formats do.  A sample file, one that can be used
for \texttt{cmr10} (and some other Computer Modern fonts) is:

\begin{verbatim}[\small]
[/Gamma/Delta/Theta/Lambda/Xi/Pi/Sigma/Upsilon
/Phi/Psi/Omega/ff/fi/fl/ffi/ffl/dotlessi
/dotlessj/grave/acute/caron/breve/macron/ring
/cedilla/germandbls/ae/oe/oslash/AE/OE/Oslash
/suppress/exclam/quotedblright/numbersign
/dollar/percent/ampersand/quoteright/parenleft
/parenright/asterisk/plus/comma/hyphen/period
/slash/zero/one/two/three/four/five/six/seven
/eight/nine/colon/semicolon/exclamdown/equal
/questiondown/question/at/A/B/C/D/E/F/G/H/I/J
/K/L/M/N/O/P/Q/R/S/T/U/V/W/X/Y/Z/bracketleft
/quotedblleft/bracketright/circumflex
/dotaccent/quoteleft/a/b/c/d/e/f/g/h/i/j/k/l
/m/n/o/p/q/r/s/t/u/v/w/x/y/z/endash/emdash
/hungarumlaut/tilde/dieresis
128{/.notdef}repeat]
\end{verbatim}

\section{Deduplicating Encodings}

The encodings inserted in the fonts do use a certain amount of
Postscript memory, and this memory usage is not presently
accounted for in the memory usage calculation of \dvips.
The memory usage is small and modern Postscript interpreters
have significant memory; further, I doubt anyone actually sets
the \dvips\ memory parameters anymore anyway.  So this is unlikely
to be an issue.  But to minimize the effect, and also to
minimize the impact on file size, encodings that are used more
than once are combined into a single instance and reused for
subsequent fonts.

\section{The \dvips\ Changes}

Almost all changes to \dvips\ are located in the single new
file \texttt{bitmapenc.c}, although a tiny bit of code was
added to \texttt{download.c} to calculate an aggregate font
bounding box, and the font description structure extended to
store this information.  I also added code to parse command
line options and configuration file options to disable or
change the behavior of the new bitmap encoding feature.

By default this feature is turned on in the new version of
\dvips.  If no encoding for a bitmapped font is found, no
change is made to the generated output for that font.

\section{Testing the Changes Without Updating}

Some may want to test my proposed changes to the \dvips\ output
files without updating their distribution or building a new
version of \dvips.  To enable this, I am providing a Perl
script called \texttt{addencoding.pl} that is in the
\texttt{type3search} repository on \texttt{github.org/rokicki}.
This script reads a Postscript file generated by \dvips\ on
standard input and writes the PostScript file that would be
generated by a modified \dvips\ on standard output.  No
additional files are required for this testing; the default
encodings for the standard \TeX\ Live fonts are built-in to
the Perl script.

\section{How to Use a Modified \dvips}

In general, \dvips\ usage is unchanged.  Warnings in
the functionality of the bitmap encoding are disabled by default,
so as to not disturb existing work flows, although this may
change in the future.

The following command line options and configuration options are
added.  The configuration option is the same as the command line
option but without the hyphen.  No configuration or command line
option presently takes an argument.

\begin{itemize}
\item \texttt{-nobitmapencoding} Disable the bitmap font encoding
functionality.
\item \texttt{-warnbitmapencoding} Enable warnings for bitmap font
encoding (such as a missing encoding for a font).
\item \texttt{-nobitmapfontscaling} Turn off the bitmap font
scaling used to pacify some \PDF\ viewers.
\PDF\ search functionality.
\item \texttt{-nobitmapfontbbox} Turn off the bitmap bounding
boxes added to pacify some \PDF\ viewers.
\end{itemize}

Use of the first or the last two options will negatively affect the
ability to search \PDF\ files generated from the resulting
PostScript.

\section{Extension Support}

Remember that the encoding file is an arbitrary Postscript fragment
that pushes a single object on the operand stack, and that object
can be a procedure.  We permit it to be a procedure to support
experimenting with other changes to the font dictionary to improve
text support in \PDF\ viewers.  For instance, if a technique for
introducing Unicode code points for glyphs into a Postscript font
dictionary is standardized and supported by various Postscript
to \PDF\ convertors, such a procedure can introduce the requisite
structures.  The procedure will not be executed until the
font dictionary for the Type 3 font is created and open.

To test this functionality, I created a \texttt{rot13.enc} file
that defines a procedure that modifies the Encoding vector to swap
single alphabetic characters much like the \texttt{rot13} obfuscation
common during the Usenet days.  With this modification, copying
text from a \PDF\ copies (mostly) content that has been obfuscated
(except for ligatures).

\bibliographystyle{tugboat}% or plain if you don't have tugboat.bst
% \nocite{book-minimal}      % just making the bibliography non-empty
% \bibliography{xampl}       % xampl.bib comes with BibTeX

\makesignature
\end{document}
